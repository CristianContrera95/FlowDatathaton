{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning Recommendations system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.layers import (\n",
    "    Input, Reshape, Flatten, Add, Activation, Lambda, Concatenate, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
    ")\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varibles\n",
    "is_test = True\n",
    "\n",
    "sample_data = 500_000 #None\n",
    "\n",
    "if is_test:\n",
    "    filter_date = dt.datetime(2021, 3, 10, 0, 0, 0).date()\n",
    "else:\n",
    "    filter_date = dt.datetime(2021, 4, 1, 0, 0, 0).date()\n",
    "\n",
    "categories_list = [\n",
    "                   'accion', 'animacion', 'animales', 'aventura', 'belico', 'biografia', 'ciencia',\n",
    "                   'ciencia ficcion', 'cocina', 'comedia', 'competencia', 'crimen', 'cultura', 'deporte',\n",
    "                   'dibujos animados', 'documental', 'drama', 'entretenimiento', 'entrevistas', 'espectaculo',\n",
    "                   'familia', 'fantasia', 'historia', 'humor', 'infantil', 'interes general', 'investigacion',\n",
    "                   'magazine', 'moda', 'musica', 'naturaleza', 'periodistico', 'policial', 'politico', 'reality',\n",
    "                   'religion', 'restauracion', 'romance', 'suspenso', 'teatro', 'terror', 'viajes', 'western'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>account_id</th>\n",
       "      <th>device_type</th>\n",
       "      <th>asset_id</th>\n",
       "      <th>tunein</th>\n",
       "      <th>resume</th>\n",
       "      <th>min_watching</th>\n",
       "      <th>tunein_hour</th>\n",
       "      <th>content_id</th>\n",
       "      <th>released_year</th>\n",
       "      <th>...</th>\n",
       "      <th>restauracion</th>\n",
       "      <th>romance</th>\n",
       "      <th>suspenso</th>\n",
       "      <th>teatro</th>\n",
       "      <th>terror</th>\n",
       "      <th>viajes</th>\n",
       "      <th>western</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>438259</th>\n",
       "      <td>27657</td>\n",
       "      <td>48596</td>\n",
       "      <td>STB</td>\n",
       "      <td>13230.0</td>\n",
       "      <td>2021-01-15 23:05:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23</td>\n",
       "      <td>2992.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jordskott</td>\n",
       "      <td>thriller,crimen,detectives</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2824123</th>\n",
       "      <td>70924</td>\n",
       "      <td>89386</td>\n",
       "      <td>STATIONARY</td>\n",
       "      <td>29950.0</td>\n",
       "      <td>2021-02-28 15:19:00</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1409.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>02/08 - corte y confeccion famosos</td>\n",
       "      <td>competencia,belleza,diseño,moda,celebridades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189254</th>\n",
       "      <td>561</td>\n",
       "      <td>26480</td>\n",
       "      <td>STB</td>\n",
       "      <td>28602.0</td>\n",
       "      <td>2021-01-15 11:06:00</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11</td>\n",
       "      <td>2163.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>gallina pintadita mini</td>\n",
       "      <td>educativo</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         customer_id  account_id device_type  asset_id              tunein  \\\n",
       "438259         27657       48596         STB   13230.0 2021-01-15 23:05:00   \n",
       "2824123        70924       89386  STATIONARY   29950.0 2021-02-28 15:19:00   \n",
       "2189254          561       26480         STB   28602.0 2021-01-15 11:06:00   \n",
       "\n",
       "         resume  min_watching  tunein_hour  content_id  released_year  ...  \\\n",
       "438259        1           2.0           23      2992.0         2017.0  ...   \n",
       "2824123       0          26.0           15      1409.0         2021.0  ...   \n",
       "2189254       0          12.0           11      2163.0         2016.0  ...   \n",
       "\n",
       "        restauracion romance suspenso teatro  terror  viajes  western  \\\n",
       "438259             0       0        0      0       0       0        0   \n",
       "2824123            0       0        0      0       0       0        0   \n",
       "2189254            0       0        0      0       0       0        0   \n",
       "\n",
       "                                      title  \\\n",
       "438259                            jordskott   \n",
       "2824123  02/08 - corte y confeccion famosos   \n",
       "2189254              gallina pintadita mini   \n",
       "\n",
       "                                             keywords  ranking  \n",
       "438259                     thriller,crimen,detectives        1  \n",
       "2824123  competencia,belleza,diseño,moda,celebridades        2  \n",
       "2189254                                     educativo       10  \n",
       "\n",
       "[3 rows x 68 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if is_test:\n",
    "    df, df_test = create_dfs(sample_data=sample_data, clean=True)  # to test\n",
    "else:\n",
    "    df, _ = create_dfs(sample_data=sample_data, ret_test=False, clean=True)  # to submit\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(data, vocab_size=200):\n",
    "    \n",
    "    # Map strngs to numbers\n",
    "    tokens = [one_hot(words, vocab_size) for words in data]\n",
    "    \n",
    "    max_len = np.max(list(map(len, tokens)))\n",
    "    \n",
    "    pad_corp = pad_sequences(tokens, maxlen=max_len, padding='post', value=0.0)\n",
    "    \n",
    "    return pad_corp, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer1D:\n",
    "    def __init__(self, n_items, n_factors):\n",
    "        self.n_items = n_items\n",
    "        self.n_factors = n_factors\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = Embedding(input_dim=self.n_items, \n",
    "                      output_dim=self.n_factors,\n",
    "                      embeddings_initializer='he_normal',\n",
    "                      embeddings_regularizer=l2(1e-6))(x)\n",
    "        x = Reshape((self.n_factors,))(x)\n",
    "#         x = Conv1D(self.n_factors, 3,  activation=\"relu\", padding=\"valid\", strides=3)(x)\n",
    "#         x = GlobalMaxPooling1D()(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmbeddingLayer2D:\n",
    "    def __init__(self, max_len, embedding_size, vocab_size):\n",
    "        self.max_len = max_len\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = Embedding(input_dim=self.vocab_size,\n",
    "                      output_dim=self.embedding_size,\n",
    "                      embeddings_initializer='he_normal',\n",
    "                      embeddings_regularizer=l2(1e-6),\n",
    "                      input_length=self.max_len)(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFilterKeras:\n",
    "    def __init__(self, users, content, rating, keywords, n_factors, \n",
    "                 embedding_size, vocab_size):\n",
    "        \n",
    "        self.n_factors = n_factors\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_len = keywords.shape[1]\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.n_users = users.nunique()\n",
    "        self.n_content = content.nunique()\n",
    "        self.min_rating = min(rating)\n",
    "        self.max_rating = max(rating)\n",
    "        # Encode ids\n",
    "        self.user_enc = LabelEncoder()\n",
    "        self.content_enc = LabelEncoder()\n",
    "        \n",
    "        self.users = self.user_enc.fit_transform(users.values)\n",
    "        self.content = self.content_enc.fit_transform(content.values)\n",
    "        \n",
    "        self.keywords = keywords.astype(np.float32)\n",
    "        self.rating = rating.values.astype(np.float32)\n",
    "\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "\n",
    "    def compile_mode(self):\n",
    "        user = Input(shape=(1,))\n",
    "        u = EmbeddingLayer1D(self.n_users, self.n_factors)(user)\n",
    "\n",
    "        content = Input(shape=(1,))\n",
    "        c = EmbeddingLayer1D(self.n_content, self.n_factors)(content)\n",
    "\n",
    "#         keyword = Input(shape=(max_len,))\n",
    "#         w = EmbeddingLayer2D(self.max_len, self.embedding_size, self.vocab_size)(keyword)\n",
    "        \n",
    "        x = Concatenate()([u, c])\n",
    "#         x = Concatenate()([x, w])\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        x = Dense(self.n_factors, \n",
    "                  activation='relu',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l1_l2()\n",
    "                 )(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(1, kernel_initializer='he_normal')(x)\n",
    "        x = Activation('sigmoid')(x)\n",
    "        x = Lambda(lambda x: x * (self.max_rating - self.min_rating) + self.min_rating)(x)\n",
    "        \n",
    "        model = Model(inputs=[user, content], outputs=x, name=\"Flow\")\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.01))\n",
    "        self.model = model\n",
    "\n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "\n",
    "    def fit_model(self, batch_size=64, epochs=10):\n",
    "        self.history = self.model.fit(x=[self.users, self.content], y=self.rating, batch_size=batch_size, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>ranking</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206510</th>\n",
       "      <td>112214</td>\n",
       "      <td>657.0</td>\n",
       "      <td>1</td>\n",
       "      <td>instituto,adolescentes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206511</th>\n",
       "      <td>112237</td>\n",
       "      <td>3377.0</td>\n",
       "      <td>1</td>\n",
       "      <td>guerra mundial,supervivencia,golden globe,viaj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206512</th>\n",
       "      <td>112254</td>\n",
       "      <td>1539.0</td>\n",
       "      <td>8</td>\n",
       "      <td>de libros,fantasia,dimensiones,40s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206513</th>\n",
       "      <td>112255</td>\n",
       "      <td>2043.0</td>\n",
       "      <td>10</td>\n",
       "      <td>droga,robo,venganza,crimen,pandillas,mafia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206514</th>\n",
       "      <td>112348</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>8</td>\n",
       "      <td>feminismo,mujeres,de libros,abusos,crimen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        account_id  content_id  ranking  \\\n",
       "206510      112214       657.0        1   \n",
       "206511      112237      3377.0        1   \n",
       "206512      112254      1539.0        8   \n",
       "206513      112255      2043.0       10   \n",
       "206514      112348      1983.0        8   \n",
       "\n",
       "                                                 keywords  \n",
       "206510                             instituto,adolescentes  \n",
       "206511  guerra mundial,supervivencia,golden globe,viaj...  \n",
       "206512                 de libros,fantasia,dimensiones,40s  \n",
       "206513         droga,robo,venganza,crimen,pandillas,mafia  \n",
       "206514          feminismo,mujeres,de libros,abusos,crimen  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_func = {\n",
    "    'ranking': 'max',\n",
    "    'keywords': lambda xs: ','.join(set(chain(*[x.split(',') for x in xs])))\n",
    "}\n",
    "\n",
    "df_views = df.groupby(['account_id', 'content_id'], \n",
    "                      as_index=False).agg(agg_func)\n",
    "\n",
    "df_views.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  6,  7,  7,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3, 16,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [10,  7,  6, 13,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 9,  1,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 20\n",
    "embeddings_size = 5\n",
    "keywords_vec, max_len = tokenize_string(df_views.keywords.values, vocab_size)\n",
    "\n",
    "keywords_vec[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Flow\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_29 (Embedding)        (None, 1, 50)        3596450     input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_30 (Embedding)        (None, 1, 50)        179700      input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_25 (Reshape)            (None, 50)           0           embedding_29[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_26 (Reshape)            (None, 50)           0           embedding_30[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 100)          0           reshape_25[0][0]                 \n",
      "                                                                 reshape_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 100)          0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 50)           5050        dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 50)           0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1)            51          dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 1)            0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 1)            0           activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,781,251\n",
      "Trainable params: 3,781,251\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "flow_model = CollaborativeFilterKeras(users=df_views.account_id, \n",
    "                                      content=df_views.content_id, \n",
    "                                      rating=df_views.ranking,\n",
    "                                      keywords=keywords_vec,\n",
    "                                      n_factors=50,\n",
    "                                      embedding_size=5,\n",
    "                                      vocab_size=vocab_size)\n",
    "\n",
    "flow_model.compile_mode()\n",
    "flow_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anybody/Projects/Acamica/venv/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "206515/206515 [==============================] - 21s 101us/step - loss: 11.3250\n",
      "Epoch 2/20\n",
      "206515/206515 [==============================] - 20s 97us/step - loss: 9.8641\n",
      "Epoch 3/20\n",
      "206515/206515 [==============================] - 21s 99us/step - loss: 8.8796\n",
      "Epoch 4/20\n",
      "206515/206515 [==============================] - 20s 95us/step - loss: 8.3275\n",
      "Epoch 5/20\n",
      "206515/206515 [==============================] - 20s 95us/step - loss: 7.9936\n",
      "Epoch 6/20\n",
      "206515/206515 [==============================] - 21s 103us/step - loss: 7.7867\n",
      "Epoch 7/20\n",
      "206515/206515 [==============================] - 21s 101us/step - loss: 7.6549\n",
      "Epoch 8/20\n",
      "206515/206515 [==============================] - 20s 99us/step - loss: 7.5361\n",
      "Epoch 9/20\n",
      "206515/206515 [==============================] - 20s 98us/step - loss: 7.4704\n",
      "Epoch 10/20\n",
      "206515/206515 [==============================] - 20s 96us/step - loss: 7.4234\n",
      "Epoch 11/20\n",
      "206515/206515 [==============================] - 23s 110us/step - loss: 7.3725\n",
      "Epoch 12/20\n",
      "206515/206515 [==============================] - 22s 109us/step - loss: 7.3348\n",
      "Epoch 13/20\n",
      "206515/206515 [==============================] - 21s 104us/step - loss: 7.2989\n",
      "Epoch 14/20\n",
      "206515/206515 [==============================] - 21s 102us/step - loss: 7.2911\n",
      "Epoch 15/20\n",
      "206515/206515 [==============================] - 20s 98us/step - loss: 7.2736\n",
      "Epoch 16/20\n",
      "206515/206515 [==============================] - 20s 97us/step - loss: 7.2410\n",
      "Epoch 17/20\n",
      "206515/206515 [==============================] - 20s 95us/step - loss: 7.2859\n",
      "Epoch 18/20\n",
      "206515/206515 [==============================] - 20s 96us/step - loss: 7.2763\n",
      "Epoch 19/20\n",
      "206515/206515 [==============================] - 21s 100us/step - loss: 7.2742\n",
      "Epoch 20/20\n",
      "206515/206515 [==============================] - 20s 98us/step - loss: 7.2486\n",
      "CPU times: user 35min 3s, sys: 1min 23s, total: 36min 26s\n",
      "Wall time: 6min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "flow_model.fit_model(batch_size=512, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendations = flow_model.predict_all(df_views[df_views.account_id.isin(df_views.account_id.values[:1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
